{
 "metadata": {
  "name": "",
  "signature": "sha256:599b7e8d98ffa7fdf6562c5784a2cbef2611a686caf2a2fcf16825943c8634c7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Text Classification in Apache Spark 1.2.1 using Python"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark import SparkContext, SparkConf\n",
      "\n",
      "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"Naive_Bayes\")\n",
      "sc   = SparkContext(conf=conf)\n",
      "\n",
      "print \"Running Spark Version %s\" % (sc.version)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Running Spark Version 1.2.1\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.mllib.feature import HashingTF\n",
      "from pyspark.mllib.regression import LabeledPoint\n",
      "from pyspark.mllib.classification import NaiveBayes\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Load positive and negative sentences from the Senetence Polarity [Dataset](http://www.cs.cornell.edu/people/pabo/movie-review-data/).\n",
      " "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#word to vector space converter, limit to 10000 words\n",
      "htf = HashingTF(10000)\n",
      "\n",
      "#let 1 - positive class, 0 - negative class\n",
      "#tokenize sentences and transform them into vector space model\n",
      "\n",
      "positiveData = sc.textFile(\"/Users/sudhir_chowbina/bigdata/spark/spark-1.2.1/data/rt-polaritydata/rt-polaritydata/rt-polarity.pos\")\n",
      "posdata = positiveData.map(lambda text : LabeledPoint(1, htf.transform(text.split(\" \"))))\n",
      "print \"No. of Positive Sentences: \" + str(posdata.count())\n",
      "posdata.persist()\n",
      "\n",
      "negativeData = sc.textFile(\"/Users/sudhir_chowbina/bigdata/spark/spark-1.2.1/data/rt-polaritydata/rt-polaritydata/rt-polarity.neg\")\n",
      "negdata = negativeData.map(lambda text : LabeledPoint(0, htf.transform(text.split(\" \"))))\n",
      "print \"No. of Negative Sentences: \" + str(negdata.count())\n",
      "negdata.persist()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "No. of Positive Sentences: 5331\n",
        "No. of Negative Sentences: 5331"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "PythonRDD[15] at RDD at PythonRDD.scala:43"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Split, Train and Calculate Prediction Labels"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Split positive and negative data 60/40 into training and test data sets\n",
      "ptrain, ptest = posdata.randomSplit([0.6, 0.4])\n",
      "ntrain, ntest = negdata.randomSplit([0.6, 0.4])\n",
      "\n",
      "#union train data with positive and negative sentences\n",
      "trainh = ptrain.union(ntrain)\n",
      "#union test data with positive and negative sentences\n",
      "testh = ptest.union(ntest)\n",
      "\n",
      "# Train a Naive Bayes model on the training data\n",
      "model = NaiveBayes.train(trainh)\n",
      "\n",
      "# Compare predicted labels to actual labels\n",
      "prediction_and_labels = testh.map(lambda point: (model.predict(point.features), point.label))\n",
      "\n",
      "# Filter to only correct predictions\n",
      "correct = prediction_and_labels.filter(lambda (predicted, actual): predicted == actual)\n",
      "\n",
      "# Calculate and print accuracy rate\n",
      "accuracy = correct.count() / float(testh.count())\n",
      "\n",
      "print \"Classifier correctly predicted category \" + str(accuracy * 100) + \" percent of the time\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Classifier correctly predicted category 71.6258033801 percent of the time\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Classification Report using Scikit-Learn"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_true = []\n",
      "y_pred = []\n",
      "for x in prediction_and_labels.collect():\n",
      "    xx = list(x)\n",
      "    \n",
      "    try:\n",
      "        tt = int(xx[1])\n",
      "        pp = int(xx[0])\n",
      "        y_true.append(tt)\n",
      "        y_pred.append(pp)\n",
      "    except:\n",
      "        continue\n",
      "        \n",
      "#http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
      "from sklearn.metrics import classification_report\n",
      "\n",
      "target_names = ['neg 0', 'pos 1']\n",
      "print classification_report(y_true, y_pred, target_names=target_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "      neg 0       0.71      0.72      0.72      2093\n",
        "      pos 1       0.72      0.71      0.72      2108\n",
        "\n",
        "avg / total       0.72      0.72      0.72      4201\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##References\n",
      "\n",
      "1. [Text classification with Apache Spark 1.1 (sentiment classification)](http://avulanov.blogspot.com/2014/08/text-classification-with-apache-spark.html)\n",
      "\n",
      "2. [MLlib - Naive Bayes](http://spark.apache.org/docs/1.2.0/mllib-naive-bayes.html)\n",
      "\n",
      "3. [PySpark API](http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.map)\n",
      "\n",
      "4. [Scikit-Learn Classification Report](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
